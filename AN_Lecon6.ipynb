{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Réseaux de neurones et Application à la reconnaissance d'images\n",
    "\n",
    "** Ines Abdeljaoued Tej, Laboratoire BIMS, IPT **\n",
    "(Avril 2017)\n",
    "\n",
    "\n",
    "Nous vous proposons un TP sur Python2/SageMath portant sur la manipulation et l'apprentissage des réseaux neuronaux. Le plan que nous allons suivre est le suivant : \n",
    "\n",
    "## 1. Brève introduction sur le perceptron\n",
    "\n",
    "Ici, nous utilisons les notions de poids synaptiques et de fonction d'activation (appelée aussi fonction de transfert). Nous montrons deux exemples de perceptron (les fonctions logiques *et* et *ou*) ainsi que la limite du perceptron. \n",
    "\n",
    "## 2. Perceptron multi-couches (PML) et Apprentissage\n",
    "\n",
    "Nous traitons l'exemple *XOR* qui nécessite une classification moins triviale que la fonction *ou*. Il s'agit de mettre en place le PML dans le but de fournir une classification des données plus pertinente. Pour cela, nous utilisons une fonction à seuil puis une sigmoide, comme fonction de mise à feu (ou fonction de transfert). \n",
    "\n",
    "## 3. Traitement de la base de données MNIST\n",
    "\n",
    "Nous proposons une étude détaillée d'un célèbre exemple tirée des travaux de Yann LeCun (Courant Institute, NYU), Corinna Cortes (Google Labs, New York) et Christopher J.C. Burges (Microsoft Research, Redmond). \n",
    "\n",
    "Un petit exemple de fonctionnement de cette base est illustré dans ce lien http://myselph.de/neuralNet.html où on peut écrire à la main un chiffre et un RN (implémenté sous matlab) permet de classifier correctement ce chiffre et donc le reconnaître. \n",
    "\n",
    "Pour reproduire les travaux de LeCun et al., nous nous inspirons fortement du code développé par Evert Nieuwenburg dans https://people.phys.ethz.ch/~evertv/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# importer les packages de calcul scientifique et numérique :\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Brève introduction sur le perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.1 La fonction logique OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "X = np.array([ [0,0],[0,1],[1,0],[1,1] ])\n",
    "print(X)\n",
    "# Output data\n",
    "y = np.array([[0,1,1,1]]).T\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
    "theta = 1\n",
    "# Fonction à seuil\n",
    "def transfert(x):\n",
    "    S = []\n",
    "    for j in x:\n",
    "        if j<theta:\n",
    "            S += [0]\n",
    "        else:\n",
    "            S += [1]\n",
    "    return(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, array([1, 1]), [0, 1, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([1,1])\n",
    "b = transfert(np.dot(X,w))\n",
    "theta, w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.2 La fonction AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
    "# Input data\n",
    "X = np.array([ [0,0],[0,1],[1,0],[1,1] ])\n",
    "\n",
    "# Output data\n",
    "y = np.array([[0,0,0,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([0.7, 0.8])\n",
    "b = transfert(np.dot(X,w))\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.3 Apprentissage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
    "# Fonction Sigmoïde\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.19967812]\n",
      " [ 11.19971835]]\n",
      "[[ 0.5       ]\n",
      " [ 0.99998632]\n",
      " [ 0.99998632]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "X = np.array([ [0,0],[0,1],[1,0],[1,1] ])\n",
    "\n",
    "# Output data\n",
    "y = np.array([[0,1,1,1]]).T\n",
    "\n",
    "w = 2*np.random.random((2,1)) - 1\n",
    "for iter in xrange(100000):\n",
    "    \n",
    "    # forward propagation\n",
    "    a = X\n",
    "    b = sigmoid(np.dot(a,w))\n",
    "    \n",
    "    # Erreur (Output désiré - Output calculé par le RN)\n",
    "    b_erreur = y - b\n",
    "\n",
    "    # Descente donnée par l'erreur multipliée * la sigmoide en b\n",
    "    b_delta = b_erreur * sigmoid(b)\n",
    "\n",
    "    # Mettre à jour les poids synaptiques\n",
    "    w += np.dot(a.T,b_delta)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Input data\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# Output data           \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Initialiser les poids synaptiques aléatoirement \n",
    "# (avec une moyenne nulle)\n",
    "w = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "def apprentissage(w,X):\n",
    "    for iter in xrange(100000):\n",
    "        # forward propagation\n",
    "        a = X\n",
    "        b = sigmoid(np.dot(a,w))\n",
    "        # Erreur (Output désiré - Output calculé par le RN)\n",
    "        b_erreur = y - b\n",
    "        # Descente donnée par l'erreur multipliée * la sigmoide en b\n",
    "        b_delta = b_erreur * sigmoid(b)\n",
    "        # Mettre à jour les poids synaptiques\n",
    "        w += np.dot(a.T,b_delta)\n",
    "    return(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 21.65460778],\n",
       "        [ -0.4054701 ],\n",
       "        [-10.23189519]]), array([[  3.60025356e-05],\n",
       "        [  2.40018587e-05],\n",
       "        [  9.99989056e-01],\n",
       "        [  9.99983584e-01]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apprentissage(w,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.4 La fonction logique *XOR*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.39533485]\n",
      " [-0.70648822]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[  6.10622664e-16],\n",
       "        [ -4.99600361e-16]]), array([[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input data\n",
    "X = np.array([ [0,0],[0,1],[1,0],[1,1] ])\n",
    "\n",
    "# Output data\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "w = 2*np.random.random((2,1)) - 1\n",
    "print w\n",
    "\n",
    "apprentissage(w,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nous constatons la limite du modèle de perceptron, avec XOR. Il est nécessaire d'utiliser des structures plus complexes, avec des couches de neurones cachées. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Perceptron multi-couches (PML) et Apprentissage\n",
    "\n",
    "Nous considérons le noyau R !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sur un RN à plusieurs couches, on effectue une recherche des meilleurs poids synaptiques, en utilisant l'algorithme de descente du gradient. Cet algorithme fonctionne de la manière suivante : \n",
    "\n",
    "$w_{i+1}=w_i+\\delta_i$ où $\\delta_i$ est l'erreur (entre activations et objectifs) multipliée par la dérivée partielle de la fonction coût. \n",
    "\n",
    "Ci-dessous, un exemple sur R (penser à changer de noyau) pour la mise en place d'un RN qui calcule la racine carré. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'library' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-09ef4cb49916>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neuralnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mXOR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInteger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInteger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInteger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInteger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxor_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInteger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInteger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInteger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInteger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnet_xor\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mneuralnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"XOR ~ Var1+Var2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxor_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mInteger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mInteger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'library' is not defined"
     ]
    }
   ],
   "source": [
    "library('neuralnet')\n",
    "XOR = c(0,1,1,0)\n",
    "xor_data = data.frame(expand.grid(c(0,1), c(0,1)), XOR)\n",
    "net_xor <- neuralnet(\"XOR ~ Var1+Var2\", xor_data, hidden=2, rep=5)\n",
    "\n",
    "xor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "f6c8ed6b74312fdf742c5cfc831cf1e2067a24c8"
     },
     "execution_count": 7,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(net_xor, rep=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Input      Output\n",
      "1 55.40665295 7.443564533\n",
      "2 24.44464157 4.944152260\n",
      "3 45.29301759 6.730008736\n",
      "4 80.84298107 8.991272494\n",
      "5 22.56408536 4.750166877\n",
      "6 69.41088187 8.331319335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              [,1]\n",
      " [1,] 0.5763467401\n",
      " [2,] 1.6885656874\n",
      " [3,] 3.0043145751\n",
      " [4,] 3.9999894164\n",
      " [5,] 4.9993899192\n",
      " [6,] 6.0017225341\n",
      " [7,] 6.9944550037\n",
      " [8,] 7.9902128872\n",
      " [9,] 9.0134750520\n",
      "[10,] 9.9528812836\n"
     ]
    },
    {
     "data": {
      "image/png": "5b1d1e9cdd1c8bc127e52c29426aa1f3746d0109"
     },
     "execution_count": 9,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install.packages('neuralnet')\n",
    "library(neuralnet)\n",
    "\n",
    "input_app <- as.data.frame(runif(50, min=0, max=100))\n",
    "output_app <- sqrt(input_app)\n",
    "data_app <- cbind(input_app, output_app)\n",
    "colnames(data_app) <- c('Input', 'Output')\n",
    "\n",
    "print(head(data_app))\n",
    "\n",
    "net.sqrt <- neuralnet(Output~Input, data_app, hidden=10, threshold=0.01)\n",
    "\n",
    "data_test <- as.data.frame((1:10)^2)\n",
    "net.results <- compute(net.sqrt, data_test)\n",
    "\n",
    "print(net.results$net.result)\n",
    "\n",
    "plot(net.sqrt, rep=\"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Traitement de la base de données MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Sur `Python3`, il est nécessaire de préciser que les divisions doivent être calculées dans $IR$ (et non dans $IQ$ comme c'est fait par défaut) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
    "#from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1 Packages nécessaires à la manipulation de la BD\n",
    "Il faut installer les packages utiles à la suite des opérations. Ce sont des packages comme `gzip` ou `pickle`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
    "#!pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# pickle et gzip permettent de télécharger \n",
    "# la base de données et la décompresser :\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "import pickle\n",
    "from pickle import load \n",
    "import gzip\n",
    "\n",
    "# Pour que les figures apparaissent dans cette feuille de travail :\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# We seed the random number generator, \n",
    "# so that we can reproducibly train the network\n",
    "import numpy as np\n",
    "np.random.seed(1990)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nous téléchargeons la base de données MNIST de Yann LeCun, qui est constituée de près de 60000 chiffres manuscrits, stockés dans des images de taille 28x28 pixels (i.e. de longueur 784). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "with gzip.open('mnist.pkl.gz', 'r') as f: \n",
    "    train_set, valid_set, test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nous restructurons la base de données, de telle sorte qu'elle corresponde à l'interface du réseau de neurones souhaité. Ici, nous avons besoin d'un input X et d'un output y où X correspond aux 784 pixels et y à un chiffre compris entre 0 et 9. \n",
    "\n",
    "La sortie est en réalité une liste de 10 nombres, comprenant des 0 et un seul 1. L'indice de ce 1 correspond au chiffre de l'image (en input). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "learn_data       = [(train_set[0][i], [1 if j == train_set[1][i] else 0 for j in range(10)]) \\\n",
    "                    for i in np.arange(len(train_set[0]))]\n",
    "test_data        = [(test_set[0][i], [1 if j == test_set[1][i] else 0 for j in range(10)]) \\\n",
    "                    for i in np.arange(len(test_set[0]))]\n",
    "validation_data  = [(valid_set[0][i], [1 if j == valid_set[1][i] else 0 for j in range(10)]) \\\n",
    "                    for i in np.arange(len(valid_set[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage object at 0x7f2b31c3d410>"
      ]
     },
     "execution_count": 8,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "25142f3973ed41625e2dcb3388516a8966881408"
     },
     "execution_count": 8,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgnr = 5555\n",
    "ax = plt\n",
    "ax.matshow( np.reshape(test_set[0][imgnr], (28,28) ), cmap=cm.gray )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage object at 0x7f4225b21990>"
      ]
     },
     "execution_count": 10,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "171f9af08072ece155ddd8332605745714fe6de7"
     },
     "execution_count": 10,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgnr = np.random.randint(0,10000)\n",
    "ax = plt\n",
    "ax.matshow( np.reshape(test_set[0][imgnr], (28,28) ), cmap=cm.gray )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 Définition de la fonction d'activation\n",
    "Nous considérons la fonction sigmoïde ainsi que sa dérivée : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
    "def sigmoid( x ):\n",
    "    return np.nan_to_num( 1/(1+np.exp(-x)) )\n",
    "\n",
    "def sigmoid_deriv( x ):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nous introduisons la fonction coût : c'est la fonction à minimiser et elle dépend des poids synaptiques. Cette minimisation fait appel à une variante de l'algorithme de descente du gradient. Nous utilisons l'algorithme de backpropagation pour entrainer le RN (ce qui est basé sur un calcul de dérivées partielles de la fonction coût par rapport aux poids synaptiques w)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
    "class QuadraticCost:\n",
    "    \"\"\" Fonction coût E(w). \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn(activations, objectifs):\n",
    "        \"\"\" Evaluation du coût quadratique. \n",
    "        C'est une optimisation classique. \"\"\"\n",
    "        return 0.5*(activations - objectifs)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def fn_deriv(activations, objectifs):\n",
    "        \"\"\" La fonction dérivée du coût. \"\"\"\n",
    "        return activations - objectifs\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(inputs, activations, objectifs):\n",
    "        \"\"\" Calcule l'erreur delta à la dernière couche \n",
    "        (output) de la fonction coût. \"\"\"\n",
    "        return (activations - objectifs)*sigmoid_deriv(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ce n'est pas l'unique manière de définir l'erreur dans un RN. Ci-dessous, il y a la fonction CrossEntropy définie comme suit : \n",
    "\n",
    "``def fn(activations, objectifs):\n",
    "    return (objectifs*log(activations) + (1-objectifs)*log(1 - activations))``\n",
    "\n",
    "La dérivée est donnée par \n",
    "\n",
    "`-(objectifs/activations - (1-objectifs)/(1-activations))`\n",
    "\n",
    "C'est le terme `activations` qui dépend de w. Nous dérivons donc ce terme dans `fn`.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 Le réseau de neurone et son apprentissage\n",
    "\n",
    "La définition de la classe neuralnetwork est développée ci-dessous par Evert Nieuwenburg. Nous supposons que le réseau est fortement connecté, dans le sens où tous les neurones d'une couche sont liés aux neurones de la couche suivante. Pour d'autres variantes de réseaux de neurones, et pour comprendre de plus près ce code, il est recommandé de visiter le site web de Michael Nielsens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class neuralnetwork:\n",
    "    \"\"\" Un réseau de neurones codé par Evert Nieuwenburg. \"\"\"\n",
    "    \n",
    "    def __init__(self, shape, cost=QuadraticCost):\n",
    "        \"\"\" Initialisation du RN \"\"\"        \n",
    "        # La taille du RN\n",
    "        self.shape = shape\n",
    "        # Nombre de couches du RN\n",
    "        self.number_of_layers = len(shape)\n",
    "        # Définir la fonction coût\n",
    "        self.cost = cost       \n",
    "        # Initialiser la marice des poids, \n",
    "        # revoir l'échelle de la Gaussienne pour donner à chaque\n",
    "        # neurone relatively peaked activation\n",
    "        self.weights = [ np.random.normal(0,1/np.sqrt(shape[i+1]),(shape[i], shape[i+1])) \\\n",
    "                          for i in range(self.number_of_layers-1) ]\n",
    "        # Initialiser les biais pour toutes les couches \n",
    "        # (en dehors de la couche des inputs)\n",
    "        self.biases  = [ np.random.normal(0,1,(shape[i])) \\\n",
    "                          for i in range(1,self.number_of_layers) ]\n",
    "            \n",
    "    def feedforward( self, inputdata ):\n",
    "        \"\"\" Feed les input à travers le RN \"\"\"\n",
    "        \n",
    "        # Fixer les inputs et outputs pour chaque couche\n",
    "        self.input_to_layer = {}\n",
    "        self.output_from_layer = {}        \n",
    "        # Pour la couche input, nous n'utilisons pas de \n",
    "        # fonction d'activation\n",
    "        self.input_to_layer[0] = inputdata\n",
    "        self.output_from_layer[0] = np.array(inputdata)    \n",
    "        # Feed les input à travers les couches\n",
    "        for layer in range(1,self.number_of_layers):\n",
    "            self.input_to_layer[layer]    = np.dot( self.output_from_layer[layer-1], \\\n",
    "                                                self.weights[layer-1] ) + self.biases[layer-1]\n",
    "            self.output_from_layer[layer] = np.array( sigmoid( self.input_to_layer[layer] ) )    \n",
    "        # Retourner l'output de la dernière couche du RN\n",
    "        return self.output_from_layer[self.number_of_layers-1]\n",
    "    \n",
    "    def backpropagate( self, targets ):\n",
    "        \"\"\" Propager l'erreur utilisée par la descente du gradient\"\"\"\n",
    "        self.delta = {}\n",
    "        self.del_cost_del_bias = {}\n",
    "        self.del_cost_del_weight = {}        \n",
    "        # Delta in dans l'output final\n",
    "        self.delta[self.number_of_layers-1] = \\\n",
    "            (self.cost).delta(self.input_to_layer[self.number_of_layers-1], \\\n",
    "            self.output_from_layer[self.number_of_layers-1], targets )\n",
    "        \n",
    "        # Calculer les deltas pour les autres couches : \n",
    "        for layer in np.arange(self.number_of_layers-2, -1, -1):          \n",
    "            self.delta[layer] = np.dot( self.delta[layer+1],  self.weights[layer].T ) * \\\n",
    "                                        sigmoid_deriv( np.array(self.input_to_layer[layer]) )\n",
    "            \n",
    "        # Calculer les dérivées partielles de l'Erreur E(w) \n",
    "        # par rapport aux biais et aux poids synaptiques \n",
    "        for layer in np.arange(self.number_of_layers-1, 0, -1):                      \n",
    "            self.del_cost_del_bias[layer]   = self.delta[layer]\n",
    "            self.del_cost_del_weight[layer] = np.dot( self.output_from_layer[layer-1].T, \\\n",
    "                                                      self.delta[layer] )\n",
    "            \n",
    "        return self.del_cost_del_bias, self.del_cost_del_weight\n",
    "    \n",
    "    def train_mini_batch( self, data, rate, l2 ):\n",
    "        \"\"\" Train the network on a mini-batch \"\"\"\n",
    "        \n",
    "        # Diviser les données en deux : input et output\n",
    "        inputs  = [ entry[0] for entry in data ]\n",
    "        objectifs = [ entry[1] for entry in data ]\n",
    "        \n",
    "        # Feed les input à travers le réseau\n",
    "        self.feedforward( inputs )\n",
    "        # Propager l'erreur backwards\n",
    "        self.backpropagate( objectifs )\n",
    "        \n",
    "        # Mettre à jour les biais et les poids\n",
    "        n = len(objectifs)\n",
    "        for layer in np.arange(1,self.number_of_layers):\n",
    "            self.biases[layer-1]  -= (rate)*np.mean(self.del_cost_del_bias[layer], axis=0)\n",
    "            self.weights[layer-1] -= (rate/n)*self.del_cost_del_weight[layer] - \\\n",
    "                                     rate*l2*self.weights[layer-1]\n",
    "    \n",
    "    def stochastic_gradient_descent( self, data, number_of_epochs, mini_batch_size, \\\n",
    "                                           rate = 1, l2 = 0.1, test_data = None ):\n",
    "        \"\"\" Apprentissage du RN en utilisant la méthode\n",
    "        de descente du gradient stochastique. \"\"\"\n",
    "        \n",
    "        # For every epoch:\n",
    "        for epoch in np.arange(number_of_epochs):\n",
    "            # Randomly split the data into mini_batches\n",
    "            np.random.shuffle(data)\n",
    "            batches = [ data[x:x+mini_batch_size] \\\n",
    "                        for x in np.arange(0, len(data), mini_batch_size) ]\n",
    "            \n",
    "            for batch in batches:\n",
    "                self.train_mini_batch( batch, rate, l2 )\n",
    "                \n",
    "            if test_data != None:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(epoch, self.evaluate(test_data), \\\n",
    "                                                           len(test_data)))\n",
    "                \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\" Evaluer la performance, en calculant \n",
    "        le nombre de tests effectué. \"\"\"\n",
    "        count = 0\n",
    "        for testcase in test_data:\n",
    "            answer = np.argmax( testcase[1] )\n",
    "            prediction = np.argmax( self.feedforward( testcase[0] ) )\n",
    "            count = count + 1 if (answer - prediction) == 0 else count\n",
    "        return count     \n",
    "                \n",
    "    def save(self, filename):\n",
    "        \"\"\" Sauvegarder les poids synaptiques \n",
    "        dans un fichier. \"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump({'biases':self.biases, 'weights':self.weights}, f )\n",
    "        \n",
    "    def load(self, filename):\n",
    "        \"\"\" Charger les poids synaptiques à partir d'un fichier. \"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Affecter les biais et les poids synaptiques\n",
    "        self.biases = data['biases']\n",
    "        self.weights = data['weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.4 Apprentissage avec l'algorithme du gradient\n",
    "\n",
    "Les images en input de la base de données MNIST data sont sous forme  28x28 pixels. Ils sont reconfigurés sous forme d'un vecteur de longueur 784. A la première couche du RN, il y a donc 784 inputs (ou neurones). La sortie (dernière couche) du RN contient 10 neurones. Nous allons entrainer un réseau avec une seule couche cachée, contenant 100 neurones. \n",
    "\n",
    "Nous entraînons le RN en 25 epochs en utilisant un mini-batch de taille 10, avec un taux d'apprentissage de 0.1 et un paramètre de régularsation L2 égal à 0.1/longueur de l'ensemble. \n",
    "Durant l'apprentissage, on évalue la performance du réseau sur la base de données de validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9064 / 10000\n",
      "Epoch 1: 9189 / 10000\n",
      "Epoch 2: 9259 / 10000\n",
      "Epoch 3: 9316 / 10000\n",
      "Epoch 4: 9354 / 10000\n",
      "Epoch 5: 9400 / 10000\n",
      "Epoch 6: 9440 / 10000\n",
      "Epoch 7: 9472 / 10000\n",
      "Epoch 8: 9483 / 10000\n",
      "Epoch 9: 9512 / 10000\n",
      "Epoch 10: 9524 / 10000\n",
      "Epoch 11: 9550 / 10000\n",
      "Epoch 12: 9547 / 10000\n",
      "Epoch 13: 9569 / 10000\n",
      "Epoch 14: 9577 / 10000\n",
      "Epoch 15: 9587 / 10000\n",
      "Epoch 16: 9595 / 10000\n",
      "Epoch 17: 9615 / 10000\n",
      "Epoch 18: 9623 / 10000\n",
      "Epoch 19: 9630 / 10000\n",
      "Epoch 20: 9634 / 10000\n",
      "Epoch 21: 9647 / 10000\n",
      "Epoch 22: 9656 / 10000\n",
      "Epoch 23: 9659 / 10000\n",
      "Epoch 24: 9679 / 10000\n"
     ]
    }
   ],
   "source": [
    "mynet = neuralnetwork( [784,100,10] )\n",
    "mynet.stochastic_gradient_descent( learn_data, 25, 10, 0.1, 0.1/len(train_set[0]), \\\n",
    "                                   test_data = validation_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
    "# Sauvegarder les poids synaptiques obtenus dans le fichier MNIST\n",
    "mynet.save(\"MNIST-QuadraticCost-Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.5 Exemples de reconnaissance d'image avec le RN\n",
    "\n",
    "Let's just look at the predictions of our network by showing the\n",
    " input image, printing the actual digit according to MNIST and including\n",
    "a plot of the output neurons of our network. Try running this many times\n",
    "and see on what kinds of digits the network does badly (usually 0 vs 6 or 3 vs 5 and 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L image numéro 395 est un 5 et notre RN a reconnu un 5\n"
     ]
    },
    {
     "data": {
      "image/png": "54f327dd5238991a0cadba2983be1cbbe623c099"
     },
     "execution_count": 53,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynet = neuralnetwork( [784,100,10] )\n",
    "mynet.load(\"MNIST-QuadraticCost-Network\")\n",
    "\n",
    "# Choisir une entrée au hasard à partir de test-data.\n",
    "imgnr = np.random.randint(0,10000)\n",
    "# Feed cet input à travers le réseau pour avoir la prévision\n",
    "prevision = mynet.feedforward( test_set[0][imgnr] )\n",
    "print(\"L image numéro {0} est un {1} et notre RN a reconnu un {2}\".format(imgnr, test_set[1][imgnr], np.argmax(prevision)))\n",
    "\n",
    "# Montrer l'image avec l'output du RN\n",
    "fig, ax = plt.subplots(1,2,figsize=(8,4))\n",
    "ax[0].matshow( np.reshape(test_set[0][imgnr], (28,28) ), cmap=cm.gray )\n",
    "ax[1].plot( prevision, lw=3 )\n",
    "ax[1].set_aspect(9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ines Abdeljaoued Tej, Labortoire BIMS, Institut Pasteur de Tunis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath (stable)",
   "language": "sagemath",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}